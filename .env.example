# =============================================================================
# Invoice Pipeline — Environment Configuration
# =============================================================================
#
# USAGE
#   1. Copy this file:   cp .env.example .env
#   2. Edit .env with your actual values
#   3. Docker Compose loads .env automatically — no extra flags needed:
#        docker compose run --rm pipeline check
#
# NOTE: .env is excluded from the Docker image build (via .dockerignore).
#       It is only read by Docker Compose at runtime on the host machine.
#       Never commit .env to version control.
# =============================================================================


# -----------------------------------------------------------------------------
# LLM — connection (OpenAI-compatible API)
# -----------------------------------------------------------------------------

# Base URL of the LLM API endpoint.
# The pipeline uses the OpenAI client, so any OpenAI-compatible backend works.
#
# Ollama on this machine (default):
#   LLM_BASE_URL=http://host.docker.internal:11434/v1
#
# Ollama on another machine:
#   LLM_BASE_URL=http://192.168.1.50:11434/v1
#
# Ollama sidecar container (when using --profile with-ollama):
#   LLM_BASE_URL=http://ollama:11434/v1
#
# OpenAI:
#   LLM_BASE_URL=https://api.openai.com/v1
#
# Groq (fast hosted inference, free tier available):
#   LLM_BASE_URL=https://api.groq.com/openai/v1
#
LLM_BASE_URL=http://host.docker.internal:11434/v1

# Model name to use for invoice extraction.
#
# Ollama (local) — recommended:
#   LLM_MODEL=qwen2.5:7b       ← best JSON accuracy
#   LLM_MODEL=llama3.2         ← good general extraction (default)
#   LLM_MODEL=mistral
#
# OpenAI:
#   LLM_MODEL=gpt-4o-mini      ← cost-effective, excellent accuracy
#   LLM_MODEL=gpt-4o
#
# Groq:
#   LLM_MODEL=llama-3.3-70b-versatile
#   LLM_MODEL=mixtral-8x7b-32768
#
LLM_MODEL=llama3.2

# API key for the LLM backend.
#
# Ollama does not require a real key — any non-empty string works:
#   LLM_API_KEY=ollama
#
# For OpenAI, Groq, or other hosted services, use your actual API key:
#   LLM_API_KEY=sk-...         (OpenAI)
#   LLM_API_KEY=gsk_...        (Groq)
#
LLM_API_KEY=ollama


# -----------------------------------------------------------------------------
# Watch / polling mode
# -----------------------------------------------------------------------------

# Selects the default processing mode when no explicit subcommand is given.
#
# false (default) — run a single batch pass over invoices/ then exit.
#                   Use with:  docker compose run --rm pipeline
#                              or a cron / scheduled task.
#
# true            — continuously poll invoices/ and process new PDFs.
#                   Use with:  docker compose up -d
#                              (optionally with restart: unless-stopped in compose)
#
# Note: explicit subcommands always override this setting:
#   docker compose run --rm pipeline process /app/invoices   ← always batch
#   docker compose run --rm pipeline watch   /app/invoices   ← always watch
#
WATCH_MODE=false

# How many seconds to wait between directory scans when running in watch mode.
#
# Lower values = faster response to new invoices, higher CPU idle usage.
# Recommended range: 15–120 seconds.
#
POLL_INTERVAL=30


# -----------------------------------------------------------------------------
# Extraction mode
# -----------------------------------------------------------------------------

# Use Docling for PDF extraction (accuracy-first, recommended).
#
# true  — DoclingExtractor: layout-aware, table-preserving, built-in OCR.
#         Downloads ~1 GB of ML models on first run (cached in a named volume).
#
# false — PlainTextExtractor: pdfplumber, fast but no table structure.
#         Use for quick tests or when Docling is unavailable.
#
USE_DOCLING=true

# -----------------------------------------------------------------------------
# Dashboard
# -----------------------------------------------------------------------------

# Port the dashboard web UI listens on (access at http://localhost:<port>)
DASHBOARD_PORT=8080

# -----------------------------------------------------------------------------
# Storage paths (optional — defaults shown, override if needed)
# -----------------------------------------------------------------------------

# SQLite database file (single file replaces per-invoice JSON output).
# All extracted data, status, and corrections are stored here.
# DB_PATH=/app/output/pipeline.db

# Directory where approved invoices are written (PDF + JSON) for backend pickup.
# The backend system should monitor this folder, consume the files, and delete them.
# EXPORT_DIR=/app/output/export
